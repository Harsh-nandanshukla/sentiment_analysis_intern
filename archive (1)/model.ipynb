{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.2.1+cu121\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is NOT available. Using CPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cvblgita/anaconda3/envs/nlp/lib/python3.9/site-packages/torch/cuda/__init__.py:141: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11040). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU is available.\")\n",
    "    print(\"Using device:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"GPU is NOT available. Using CPU.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext import data\n",
    "from torchtext import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Merged dataset size: 55328\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"Sarcasm_Headlines_Dataset_v2.json\", \"r\") as f1, open(\"Sarcasm_Headlines_Dataset.json\", \"r\") as f2:\n",
    "    data1 = [json.loads(line) for line in f1]\n",
    "    data2 = [json.loads(line) for line in f2]\n",
    "\n",
    "merged_data = data1 + data2\n",
    "\n",
    "with open(\"merged_dataset.json\", \"w\") as f:\n",
    "    for item in merged_data:\n",
    "        f.write(json.dumps(item) + \"\\n\")\n",
    "\n",
    "print(\" Merged dataset size:\", len(merged_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load JSON Lines file\n",
    "with open(\"merged_dataset.json\", \"r\") as f:\n",
    "    raw_data = [json.loads(line) for line in f]\n",
    "\n",
    "# Use spaCy tokenizer\n",
    "spacy_en = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def tokenize(text):\n",
    "    return [tok.text.lower() for tok in spacy_en.tokenizer(text)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader  \n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torchtext.vocab import build_vocab_from_iterator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SarcasmDataset(Dataset):\n",
    "    def __init__(self, data, vocab=None):\n",
    "        self.texts = [tokenize(item[\"headline\"]) for item in data]\n",
    "        self.labels = [item[\"is_sarcastic\"] for item in data]\n",
    "\n",
    "        if vocab is None:\n",
    "            self.vocab = build_vocab_from_iterator(self.texts, specials=[\"<pad>\", \"<unk>\"])\n",
    "            self.vocab.set_default_index(self.vocab[\"<unk>\"])\n",
    "        else:\n",
    "            self.vocab = vocab\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tokens = self.vocab(self.texts[idx])\n",
    "        return torch.tensor(tokens), torch.tensor(self.labels[idx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    texts, labels = zip(*batch)\n",
    "    lengths = torch.tensor([len(x) for x in texts])\n",
    "    padded_texts = pad_sequence(texts, batch_first=True, padding_value=0)\n",
    "    labels = torch.tensor(labels)\n",
    "    return padded_texts, lengths, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SarcasmDataset(raw_data)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full dataset size: 55328\n"
     ]
    }
   ],
   "source": [
    "print(\"Full dataset size:\", len(raw_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 44262\n",
      "Test size : 11066\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming `merged_data` is already loaded as a list of dicts\n",
    "train_data, test_data = train_test_split(merged_data, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Train size:\", len(train_data))\n",
    "print(\"Test size :\", len(test_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SarcasmDataset(train_data)\n",
    "test_dataset = SarcasmDataset(test_data, vocab=train_dataset.vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, padding_idx):\n",
    "        super(GRUClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=padding_idx)\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, text, lengths):\n",
    "        embedded = self.embedding(text)\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(embedded, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        _, hidden = self.gru(packed)\n",
    "        return self.fc(self.dropout(hidden[-1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = GRUClassifier(\n",
    "    vocab_size=len(train_dataset.vocab),\n",
    "    embedding_dim=100,\n",
    "    hidden_dim=64,\n",
    "    output_dim=2,\n",
    "    padding_idx=train_dataset.vocab['<pad>']\n",
    ").to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    for texts, lengths, labels in dataloader:\n",
    "        texts, lengths, labels = texts.to(device), lengths.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(texts, lengths)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        all_preds.extend(torch.argmax(outputs, dim=1).cpu().tolist())\n",
    "        all_labels.extend(labels.cpu().tolist())\n",
    "\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='binary')\n",
    "    return total_loss / len(dataloader), acc, precision, recall, f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for texts, lengths, labels in dataloader:\n",
    "            texts, lengths, labels = texts.to(device), lengths.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(texts, lengths)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            all_preds.extend(torch.argmax(outputs, dim=1).cpu().tolist())\n",
    "            all_labels.extend(labels.cpu().tolist())\n",
    "\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='binary')\n",
    "    return total_loss / len(dataloader), acc, precision, recall, f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/50\n",
      "Train  | Loss: 0.0406 | Acc: 0.9873 | Prec: 0.9869 | Rec: 0.9852 | F1: 0.9861\n",
      "Test   | Loss: 0.2146  | Acc: 0.9500  | Prec: 0.9575  | Rec: 0.9329  | F1: 0.9450\n",
      "\n",
      "Epoch 2/50\n",
      "Train  | Loss: 0.0223 | Acc: 0.9930 | Prec: 0.9926 | Rec: 0.9920 | F1: 0.9923\n",
      "Test   | Loss: 0.2353  | Acc: 0.9546  | Prec: 0.9605  | Rec: 0.9401  | F1: 0.9502\n",
      "\n",
      "Epoch 3/50\n",
      "Train  | Loss: 0.0140 | Acc: 0.9954 | Prec: 0.9952 | Rec: 0.9948 | F1: 0.9950\n",
      "Test   | Loss: 0.2590  | Acc: 0.9586  | Prec: 0.9679  | Rec: 0.9413  | F1: 0.9544\n",
      "\n",
      "Epoch 4/50\n",
      "Train  | Loss: 0.0114 | Acc: 0.9965 | Prec: 0.9961 | Rec: 0.9962 | F1: 0.9962\n",
      "Test   | Loss: 0.2795  | Acc: 0.9549  | Prec: 0.9690  | Rec: 0.9319  | F1: 0.9501\n",
      "\n",
      "Epoch 5/50\n",
      "Train  | Loss: 0.0067 | Acc: 0.9983 | Prec: 0.9981 | Rec: 0.9981 | F1: 0.9981\n",
      "Test   | Loss: 0.2835  | Acc: 0.9580  | Prec: 0.9740  | Rec: 0.9337  | F1: 0.9534\n",
      "\n",
      "Epoch 6/50\n",
      "Train  | Loss: 0.0084 | Acc: 0.9976 | Prec: 0.9972 | Rec: 0.9975 | F1: 0.9974\n",
      "Test   | Loss: 0.2746  | Acc: 0.9588  | Prec: 0.9631  | Rec: 0.9468  | F1: 0.9549\n",
      "\n",
      "Epoch 7/50\n",
      "Train  | Loss: 0.0054 | Acc: 0.9984 | Prec: 0.9981 | Rec: 0.9983 | F1: 0.9982\n",
      "Test   | Loss: 0.3007  | Acc: 0.9583  | Prec: 0.9721  | Rec: 0.9362  | F1: 0.9538\n",
      "\n",
      "Epoch 8/50\n",
      "Train  | Loss: 0.0029 | Acc: 0.9991 | Prec: 0.9990 | Rec: 0.9991 | F1: 0.9990\n",
      "Test   | Loss: 0.3243  | Acc: 0.9572  | Prec: 0.9676  | Rec: 0.9384  | F1: 0.9528\n",
      "\n",
      "Epoch 9/50\n",
      "Train  | Loss: 0.0065 | Acc: 0.9980 | Prec: 0.9976 | Rec: 0.9980 | F1: 0.9978\n",
      "Test   | Loss: 0.3040  | Acc: 0.9564  | Prec: 0.9718  | Rec: 0.9323  | F1: 0.9516\n",
      "\n",
      "Epoch 10/50\n",
      "Train  | Loss: 0.0047 | Acc: 0.9986 | Prec: 0.9987 | Rec: 0.9982 | F1: 0.9985\n",
      "Test   | Loss: 0.2881  | Acc: 0.9604  | Prec: 0.9680  | Rec: 0.9453  | F1: 0.9565\n",
      "\n",
      "Epoch 11/50\n",
      "Train  | Loss: 0.0036 | Acc: 0.9990 | Prec: 0.9990 | Rec: 0.9988 | F1: 0.9989\n",
      "Test   | Loss: 0.2882  | Acc: 0.9588  | Prec: 0.9670  | Rec: 0.9427  | F1: 0.9547\n",
      "\n",
      "Epoch 12/50\n",
      "Train  | Loss: 0.0038 | Acc: 0.9990 | Prec: 0.9988 | Rec: 0.9990 | F1: 0.9989\n",
      "Test   | Loss: 0.3139  | Acc: 0.9592  | Prec: 0.9589  | Rec: 0.9523  | F1: 0.9556\n",
      "\n",
      "Epoch 13/50\n",
      "Train  | Loss: 0.0036 | Acc: 0.9991 | Prec: 0.9989 | Rec: 0.9991 | F1: 0.9990\n",
      "Test   | Loss: 0.3407  | Acc: 0.9594  | Prec: 0.9701  | Rec: 0.9409  | F1: 0.9553\n",
      "\n",
      "Epoch 14/50\n",
      "Train  | Loss: 0.0022 | Acc: 0.9993 | Prec: 0.9993 | Rec: 0.9991 | F1: 0.9992\n",
      "Test   | Loss: 0.3326  | Acc: 0.9582  | Prec: 0.9619  | Rec: 0.9466  | F1: 0.9542\n",
      "\n",
      "Epoch 15/50\n",
      "Train  | Loss: 0.0025 | Acc: 0.9994 | Prec: 0.9994 | Rec: 0.9993 | F1: 0.9993\n",
      "Test   | Loss: 0.3570  | Acc: 0.9589  | Prec: 0.9695  | Rec: 0.9403  | F1: 0.9547\n",
      "\n",
      "Epoch 16/50\n",
      "Train  | Loss: 0.0029 | Acc: 0.9991 | Prec: 0.9992 | Rec: 0.9990 | F1: 0.9991\n",
      "Test   | Loss: 0.3571  | Acc: 0.9600  | Prec: 0.9693  | Rec: 0.9429  | F1: 0.9559\n",
      "\n",
      "Epoch 17/50\n",
      "Train  | Loss: 0.0010 | Acc: 0.9997 | Prec: 0.9997 | Rec: 0.9997 | F1: 0.9997\n",
      "Test   | Loss: 0.3704  | Acc: 0.9593  | Prec: 0.9689  | Rec: 0.9419  | F1: 0.9552\n",
      "\n",
      "Epoch 18/50\n",
      "Train  | Loss: 0.0016 | Acc: 0.9995 | Prec: 0.9995 | Rec: 0.9995 | F1: 0.9995\n",
      "Test   | Loss: 0.3479  | Acc: 0.9570  | Prec: 0.9556  | Rec: 0.9507  | F1: 0.9532\n",
      "\n",
      "Epoch 19/50\n",
      "Train  | Loss: 0.0027 | Acc: 0.9992 | Prec: 0.9991 | Rec: 0.9991 | F1: 0.9991\n",
      "Test   | Loss: 0.3551  | Acc: 0.9591  | Prec: 0.9674  | Rec: 0.9429  | F1: 0.9550\n",
      "\n",
      "Epoch 20/50\n",
      "Train  | Loss: 0.0017 | Acc: 0.9995 | Prec: 0.9996 | Rec: 0.9993 | F1: 0.9994\n",
      "Test   | Loss: 0.3763  | Acc: 0.9582  | Prec: 0.9653  | Rec: 0.9431  | F1: 0.9540\n",
      "\n",
      "Epoch 21/50\n",
      "Train  | Loss: 0.0018 | Acc: 0.9996 | Prec: 0.9997 | Rec: 0.9996 | F1: 0.9996\n",
      "Test   | Loss: 0.3514  | Acc: 0.9594  | Prec: 0.9657  | Rec: 0.9454  | F1: 0.9555\n",
      "\n",
      "Epoch 22/50\n",
      "Train  | Loss: 0.0020 | Acc: 0.9996 | Prec: 0.9995 | Rec: 0.9996 | F1: 0.9995\n",
      "Test   | Loss: 0.3679  | Acc: 0.9592  | Prec: 0.9681  | Rec: 0.9423  | F1: 0.9551\n",
      "\n",
      "Epoch 23/50\n",
      "Train  | Loss: 0.0002 | Acc: 1.0000 | Prec: 1.0000 | Rec: 1.0000 | F1: 1.0000\n",
      "Test   | Loss: 0.3821  | Acc: 0.9601  | Prec: 0.9682  | Rec: 0.9443  | F1: 0.9561\n",
      "\n",
      "Epoch 24/50\n",
      "Train  | Loss: 0.0000 | Acc: 1.0000 | Prec: 1.0000 | Rec: 1.0000 | F1: 1.0000\n",
      "Test   | Loss: 0.4036  | Acc: 0.9603  | Prec: 0.9697  | Rec: 0.9433  | F1: 0.9563\n",
      "\n",
      "Epoch 25/50\n",
      "Train  | Loss: 0.0001 | Acc: 1.0000 | Prec: 1.0000 | Rec: 1.0000 | F1: 1.0000\n",
      "Test   | Loss: 0.4357  | Acc: 0.9607  | Prec: 0.9686  | Rec: 0.9453  | F1: 0.9568\n",
      "\n",
      "Epoch 26/50\n",
      "Train  | Loss: 0.0000 | Acc: 1.0000 | Prec: 1.0000 | Rec: 1.0000 | F1: 1.0000\n",
      "Test   | Loss: 0.4601  | Acc: 0.9604  | Prec: 0.9686  | Rec: 0.9447  | F1: 0.9565\n",
      "\n",
      "Epoch 27/50\n",
      "Train  | Loss: 0.0000 | Acc: 1.0000 | Prec: 1.0000 | Rec: 1.0000 | F1: 1.0000\n",
      "Test   | Loss: 0.4846  | Acc: 0.9603  | Prec: 0.9690  | Rec: 0.9441  | F1: 0.9564\n",
      "\n",
      "Epoch 28/50\n",
      "Train  | Loss: 0.0000 | Acc: 1.0000 | Prec: 1.0000 | Rec: 1.0000 | F1: 1.0000\n",
      "Test   | Loss: 0.5106  | Acc: 0.9607  | Prec: 0.9701  | Rec: 0.9437  | F1: 0.9567\n",
      "\n",
      "Epoch 29/50\n",
      "Train  | Loss: 0.0034 | Acc: 0.9995 | Prec: 0.9994 | Rec: 0.9995 | F1: 0.9994\n",
      "Test   | Loss: 0.4108  | Acc: 0.9534  | Prec: 0.9716  | Rec: 0.9258  | F1: 0.9482\n",
      "\n",
      "Epoch 30/50\n",
      "Train  | Loss: 0.0046 | Acc: 0.9987 | Prec: 0.9988 | Rec: 0.9984 | F1: 0.9986\n",
      "Test   | Loss: 0.3544  | Acc: 0.9589  | Prec: 0.9704  | Rec: 0.9394  | F1: 0.9546\n",
      "\n",
      "Epoch 31/50\n",
      "Train  | Loss: 0.0009 | Acc: 0.9998 | Prec: 0.9997 | Rec: 0.9999 | F1: 0.9998\n",
      "Test   | Loss: 0.3514  | Acc: 0.9598  | Prec: 0.9712  | Rec: 0.9405  | F1: 0.9556\n",
      "\n",
      "Epoch 32/50\n",
      "Train  | Loss: 0.0001 | Acc: 1.0000 | Prec: 1.0000 | Rec: 1.0000 | F1: 1.0000\n",
      "Test   | Loss: 0.3812  | Acc: 0.9595  | Prec: 0.9704  | Rec: 0.9407  | F1: 0.9554\n",
      "\n",
      "Epoch 33/50\n",
      "Train  | Loss: 0.0001 | Acc: 1.0000 | Prec: 1.0000 | Rec: 1.0000 | F1: 1.0000\n",
      "Test   | Loss: 0.3784  | Acc: 0.9574  | Prec: 0.9686  | Rec: 0.9380  | F1: 0.9530\n",
      "\n",
      "Epoch 34/50\n",
      "Train  | Loss: 0.0019 | Acc: 0.9994 | Prec: 0.9994 | Rec: 0.9994 | F1: 0.9994\n",
      "Test   | Loss: 0.3460  | Acc: 0.9583  | Prec: 0.9688  | Rec: 0.9396  | F1: 0.9540\n",
      "\n",
      "Epoch 35/50\n",
      "Train  | Loss: 0.0014 | Acc: 0.9995 | Prec: 0.9995 | Rec: 0.9996 | F1: 0.9995\n",
      "Test   | Loss: 0.3699  | Acc: 0.9592  | Prec: 0.9700  | Rec: 0.9405  | F1: 0.9551\n",
      "\n",
      "Epoch 36/50\n",
      "Train  | Loss: 0.0000 | Acc: 1.0000 | Prec: 1.0000 | Rec: 1.0000 | F1: 1.0000\n",
      "Test   | Loss: 0.3912  | Acc: 0.9594  | Prec: 0.9697  | Rec: 0.9413  | F1: 0.9553\n",
      "\n",
      "Epoch 37/50\n",
      "Train  | Loss: 0.0000 | Acc: 1.0000 | Prec: 1.0000 | Rec: 1.0000 | F1: 1.0000\n",
      "Test   | Loss: 0.4055  | Acc: 0.9599  | Prec: 0.9705  | Rec: 0.9415  | F1: 0.9558\n",
      "\n",
      "Epoch 38/50\n",
      "Train  | Loss: 0.0000 | Acc: 1.0000 | Prec: 1.0000 | Rec: 1.0000 | F1: 1.0000\n",
      "Test   | Loss: 0.4266  | Acc: 0.9598  | Prec: 0.9697  | Rec: 0.9421  | F1: 0.9557\n",
      "\n",
      "Epoch 39/50\n",
      "Train  | Loss: 0.0000 | Acc: 1.0000 | Prec: 1.0000 | Rec: 1.0000 | F1: 1.0000\n",
      "Test   | Loss: 0.4496  | Acc: 0.9594  | Prec: 0.9693  | Rec: 0.9417  | F1: 0.9553\n",
      "\n",
      "Epoch 40/50\n",
      "Train  | Loss: 0.0000 | Acc: 1.0000 | Prec: 1.0000 | Rec: 1.0000 | F1: 1.0000\n",
      "Test   | Loss: 0.4741  | Acc: 0.9592  | Prec: 0.9689  | Rec: 0.9417  | F1: 0.9551\n",
      "\n",
      "Epoch 41/50\n",
      "Train  | Loss: 0.0010 | Acc: 0.9998 | Prec: 0.9998 | Rec: 0.9998 | F1: 0.9998\n",
      "Test   | Loss: 0.4245  | Acc: 0.9569  | Prec: 0.9678  | Rec: 0.9376  | F1: 0.9525\n",
      "\n",
      "Epoch 42/50\n",
      "Train  | Loss: 0.0049 | Acc: 0.9986 | Prec: 0.9986 | Rec: 0.9984 | F1: 0.9985\n",
      "Test   | Loss: 0.3604  | Acc: 0.9579  | Prec: 0.9682  | Rec: 0.9394  | F1: 0.9536\n",
      "\n",
      "Epoch 43/50\n",
      "Train  | Loss: 0.0004 | Acc: 0.9999 | Prec: 0.9999 | Rec: 0.9999 | F1: 0.9999\n",
      "Test   | Loss: 0.3731  | Acc: 0.9573  | Prec: 0.9626  | Rec: 0.9441  | F1: 0.9532\n",
      "\n",
      "Epoch 44/50\n",
      "Train  | Loss: 0.0001 | Acc: 1.0000 | Prec: 1.0000 | Rec: 1.0000 | F1: 1.0000\n",
      "Test   | Loss: 0.3913  | Acc: 0.9566  | Prec: 0.9620  | Rec: 0.9431  | F1: 0.9524\n",
      "\n",
      "Epoch 45/50\n",
      "Train  | Loss: 0.0000 | Acc: 1.0000 | Prec: 1.0000 | Rec: 1.0000 | F1: 1.0000\n",
      "Test   | Loss: 0.4082  | Acc: 0.9579  | Prec: 0.9665  | Rec: 0.9411  | F1: 0.9537\n",
      "\n",
      "Epoch 46/50\n",
      "Train  | Loss: 0.0000 | Acc: 1.0000 | Prec: 1.0000 | Rec: 1.0000 | F1: 1.0000\n",
      "Test   | Loss: 0.4244  | Acc: 0.9577  | Prec: 0.9654  | Rec: 0.9419  | F1: 0.9535\n",
      "\n",
      "Epoch 47/50\n",
      "Train  | Loss: 0.0000 | Acc: 1.0000 | Prec: 1.0000 | Rec: 1.0000 | F1: 1.0000\n",
      "Test   | Loss: 0.4463  | Acc: 0.9576  | Prec: 0.9665  | Rec: 0.9405  | F1: 0.9534\n",
      "\n",
      "Epoch 48/50\n",
      "Train  | Loss: 0.0000 | Acc: 1.0000 | Prec: 1.0000 | Rec: 1.0000 | F1: 1.0000\n",
      "Test   | Loss: 0.4683  | Acc: 0.9577  | Prec: 0.9684  | Rec: 0.9388  | F1: 0.9534\n",
      "\n",
      "Epoch 49/50\n",
      "Train  | Loss: 0.0000 | Acc: 1.0000 | Prec: 1.0000 | Rec: 1.0000 | F1: 1.0000\n",
      "Test   | Loss: 0.4907  | Acc: 0.9581  | Prec: 0.9681  | Rec: 0.9400  | F1: 0.9538\n",
      "\n",
      "Epoch 50/50\n",
      "Train  | Loss: 0.0000 | Acc: 1.0000 | Prec: 1.0000 | Rec: 1.0000 | F1: 1.0000\n",
      "Test   | Loss: 0.5121  | Acc: 0.9578  | Prec: 0.9673  | Rec: 0.9401  | F1: 0.9535\n",
      "\n",
      "Average Train Metrics:\n",
      "Loss: 0.0034 | Acc: 0.9990 | Prec: 0.9989 | Rec: 0.9989 | F1: 0.9989\n",
      "\n",
      "Average Test Metrics:\n",
      "Loss: 0.3727 | Acc: 0.9583 | Prec: 0.9675 | Rec: 0.9411 | F1: 0.9541\n"
     ]
    }
   ],
   "source": [
    "train_metrics = []\n",
    "test_metrics = []\n",
    "Epochs=50\n",
    "for epoch in range(Epochs):\n",
    "    train_loss, train_acc, train_prec, train_rec, train_f1 = train_epoch(model, train_loader, optimizer, criterion)\n",
    "    test_loss, test_acc, test_prec, test_rec, test_f1 = evaluate(model, test_loader, criterion)\n",
    "\n",
    "    train_metrics.append((train_loss, train_acc, train_prec, train_rec, train_f1))\n",
    "    test_metrics.append((test_loss, test_acc, test_prec, test_rec, test_f1))\n",
    "\n",
    "    print(f\"\\nEpoch {epoch+1}/{Epochs}\")\n",
    "    print(f\"Train  | Loss: {train_loss:.4f} | Acc: {train_acc:.4f} | Prec: {train_prec:.4f} | Rec: {train_rec:.4f} | F1: {train_f1:.4f}\")\n",
    "    print(f\"Test   | Loss: {test_loss:.4f}  | Acc: {test_acc:.4f}  | Prec: {test_prec:.4f}  | Rec: {test_rec:.4f}  | F1: {test_f1:.4f}\")\n",
    "\n",
    "#  Average metrics\n",
    "import numpy as np\n",
    "train_metrics = np.array(train_metrics)\n",
    "test_metrics = np.array(test_metrics)\n",
    "\n",
    "avg_train = train_metrics.mean(axis=0)\n",
    "avg_test = test_metrics.mean(axis=0)\n",
    "\n",
    "print(\"\\nAverage Train Metrics:\")\n",
    "print(f\"Loss: {avg_train[0]:.4f} | Acc: {avg_train[1]:.4f} | Prec: {avg_train[2]:.4f} | Rec: {avg_train[3]:.4f} | F1: {avg_train[4]:.4f}\")\n",
    "\n",
    "print(\"\\nAverage Test Metrics:\")\n",
    "print(f\"Loss: {avg_test[0]:.4f} | Acc: {avg_test[1]:.4f} | Prec: {avg_test[2]:.4f} | Rec: {avg_test[3]:.4f} | F1: {avg_test[4]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
